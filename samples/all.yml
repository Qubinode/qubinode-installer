---
######################################
#         USER VARIABLES             #
# The are autojatically updated or   #
# you can update them manually       #
######################################
inventory_dir: ""
project_dir: ""
admin_user: ""
domain: ""

# PTR - The PTR record that should be use
qubinode_ptr: changeme.in-addr.arpa

# Set this to false if the host is already setup to function as a KVM host
# Set to true if this box needs to be setup as a KVM host.
run_qubinode_setup: ""

# Public DNS server
dns_forwarder: ""

# ROLE: swygue-redhat-subscription
# whether to use username/pass or RHSM
rhsm_reg_method: ""



######################################
#         SYSTEM VARIABLES           #
# You shouldn't need to change these #
######################################

# RHEL 7.7 is the current tested RHEL 7 minor release.
# RHEL 7.6 was also tested and may work if you need to stay at 7.6
rhel_release: 7.7

# Ansible 2.6 is current working version
# Ansible 2.7 is the current tested version.
# Ansible 2.8 is currently being tested.
ansible_repo: rhel-7-server-ansible-2.6-rpms
ansible_version: 2.6.20
ansible_release: 2.6
ansible_rpm: ansible-2.6.20-1.el7ae.noarch

# We leverage a bridge network for OCP3 installs
# and other VMS. This sets the name of the bridge to be created and use when
# deploying VMS. If there is an existing libvirt bridge network, set the name here instea.
qubinode_bridge_name: qubibr0

# All VMs created name will begin with this prefix.
instance_prefix: qbn
preappend_host_name: "{{ instance_prefix }}-{{ product }}-"

# should a bridge interface be created
configure_bridge: true

# Set to no prevent the installer from attempting
# setup a LVM group for qubinode. Also set this to no
# if you already have you storage for lvm setup
create_lvm: yes

vm_public_key: "/home/{{ admin_user }}/.ssh/id_rsa.pub"
update_inventory: true
vm_data_dir: "/var/lib/vmdata"
inventory_file: "{{ inventory_dir }}/hosts"
admin_user_password: "{{ admin_user_password }}"
kvm_vm_root_pwd: "{{ admin_user_password }}"
vm_domain: "{{ domain }}"
search_domains:
  - "{{ vm_domain }}"
# This is for /etc/resolv.conf
dns_servers:
  - "{{ idm_server_ip | default('1.1.1.1') }}"
  - 8.8.8.8

public_domain: "{{ domain }}"

qubinode_installer_setup_completed: no
qubinode_installer_rhsm_completed: no
qubinode_installer_ansible_completed: no
qubinode_installer_host_completed: no
qubinode_base_reqs_completed: no

# use by funciton check_hardware_resources
libvirt_pool_name: default

# Ensure VMs are always using static IP even when DHCP issued
static_ip: true

# ROLE: swygue-redhat-subscription
rhsm_org: "{{ rhsm_org }}"
rhsm_activationkey: "{{ rhsm_activationkey }}"
rhsm_org_id: "{{ rhsm_org }}"
rhsm_setup_insights_client: false
rhsm_user: "{{ rhsm_username }}"
rhsm_pass: "{{ rhsm_password }}"
org_id: "{{ rhsm_org }}"

# ROLE: swygue.edge_host_setup role
# the user that will admin the system
ssh_username: "{{ admin_user }}"

# ROLE: deploy-kvm-vm
#--------------------
# name of the libvirt network to use
vm_libvirt_net: "qubinet"
# Use to create VM
# Red Hat Enterprise Linux 7.7 Update KVM Guest Image (20191016)
os_qcow_image_name: rhel-server-7.7-update-2-x86_64-kvm.qcow2
cloud_init_vm_image: "{{ os_qcow_image_name }}"

# setting this to true will result is no VMs being created and any existing vms to be deleted
vm_teardown: False

ipa_client_principal: "{{ idm_admin_user }}"
ipa_client_pass: "{{ idm_dm_pwd }}"
ipa_host: "{{ idm_hostname }}.{{ domain }}"
ipa_client_install_flags: "--server='{{ ipa_host }}' --realm='{{ idm_realm }}' --mkhomedir --enable-dns-updates -N --domain='{{ domain }}'"

 # LVM
logical_volumes:
  - name: qubi_images
    size: "+100%FREE"
    mount_dir: "{{ kvm_host_libvirt_dir | default('/var/lib/libvirt/images') }}"
    fstype: xfs


#################################
# KVM host / Qubinode variables #
#################################

# set path to libvirt images
kvm_host_libvirt_dir: /var/lib/libvirt/images
host_device: "{{ kvm_host_libvirt_extra_disk | default('nvme0n1') }}"
vg_name: vg_qubi
vm_libvirt_net_check: yes
libvirt_pool_name_check: yes

# Hardware profiles
qubinode_minimal_memory: 30
qubinode_standard_memory: 80
qubinode_performance_memory: 88

# We are not enforcing acutal sizes at this time
qubinode_minimal_storage: 370      # actual is 640
qubinode_standard_storage: 900     # actual is 1160
qubinode_performance_storage: 1340 # actual is 1340

# your system profile
storage_profile: ""
memory_profile: ""

###########################################
# variables for swygue.edge_host_setup role


# This is required for OpenShift 4
# TODO: the codebase needs to up to look for this
# variable under playbooks/var/ocp4.yml
cluster_name: ocp42

nat_network_name: "{{ cluster_name }}"
libvirt_host_networks:
  - name: "{{ nat_network_name }}"
    create: true
    mode: nat
    int_domain: compute.local
    external_domain: "{{ public_domain }}"
    mac_start: "52:54:00:"
    master_count: 3
    compute_count: 2
    subnet: "192.168.50.0"
    gateway: "192.168.50.1"
    mask: "255.255.255.0"
  - name: "{{ vm_libvirt_net }}"
    create: true
    mode: bridge
    bridge_device: "{{ kvm_host_bridge_name | default(qubinode_bridge_name) }}"
    ifcfg_type: "{{ kvm_bridge_type }}"
    ifcfg_bootproto: "{{ kvm_host_bootproto }}"
    bridge_slave_dev: "{{ kvm_host_interface }}"
    gateway: "{{ kvm_host_gw }}"
    mask_prefix: "{{ kvm_host_mask_prefix }}"
    ipaddress: "{{ kvm_host_ip }}"
    mask: "{{ kvm_host_netmask }}"
    mac: "{{ kvm_host_macaddr }}"

#This is for KVM host initial setup of /etc/resolv.conf
kvm_host_dns_server: "{{ idm_server_ip | default('1.1.1.1') }}"
kvm_host_ip: ""
kvm_host_interface: ""
kvm_host_gw: ""
kvm_host_netmask: ""
kvm_host_macaddr: ""
kvm_bridge_type: Bridge
kvm_host_bootproto: none
kvm_host_mask_prefix: ""
kvm_host_domain: "{{ domain }}"

libvirt_pkgs:
   - virt-install
   - libvirt-daemon-config-network
   - libvirt-daemon-kvm
   - libguestfs-tools
   - libvirt-client
   - qemu-kvm
   - nfs-utils
   - libvirt-daemon
   - libvirt-client
   - virt-top
   - tuned
   - openssh-server
   - wget
   - git
   - net-tools
   - bind-utils
   - yum-utils
   - iptables-services
   - bridge-utils
   - bash-completion
   - kexec-tools
   - sos
   - psacct
   - vim
   - pyOpenSSL
   - device-mapper-event-libs
   - device-mapper-libs
   - httpd-tools
   - java-1.8.0-openjdk-devel.x86_64
   - tmux
   - python-dns

# used in playbook setup_kvmhost.yml
network_interface_name: "{{ kvm_host_interface }}"

###########################
# General Networking Info #
###########################

####################
## VM Network Info
##-------------------
## Currently we assume the VMs will use the name network as the host
## This is why a bridge network is created by default. These values
## can be changed to reflect a different network.
vm_net_gateway: "{{ kvm_host_gw }}"
vm_net_netmask: "{{ kvm_host_netmask }}"
vm_gateway: "{{ vm_net_gateway }}"


###########################
# these should be deleted #
# #########################

# KVM host packages
qubi_required_pkgs: |
   "virt-install, libvirt-daemon-config-network, libvirt-daemon-kvm,
   libguestfs-tools, libvirt-client, qemu-kvm, nfs-utils, libvirt-daemon,
   libvirt-client, virt-top, cockpit, cockpit-networkmanager, cockpit-dashboard,
   cockpit-storaged, cockpit-packagekit, cockpit-machines, cockpit-sosreport,
   cockpit-kubernetes, cockpit-pcp, cockpit-bridge, tuned, openssh-server,
   wget, git, net-tools, bind-utils, yum-utils, iptables-services, bridge-utils,
   bash-completion, kexec-tools, sos, psacct, vim, ansible,
   pyOpenSSL, docker, device-mapper-event-libs, device-mapper-libs, httpd-tools, tmux, patch,  python-netaddr"
